{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 2 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from helpers.preprocessing import *\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import chemsource as cs\n",
    "\n",
    "pandarallel.initialize(nb_workers=2, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retrieval model\n",
    "ncbi_key = pd.read_csv(\"../secrets/ncbi_api.txt\", header=None).values[0][0]\n",
    "model = cs.ChemSource(ncbi_key=ncbi_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb059b02c6a4c32a4b7b395f03c788b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2979), Label(value='0 / 2979'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0e10f5b6c44879adf0eb3950ad2616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2877), Label(value='0 / 2877'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajitrr/miniconda3/envs/chemsource/lib/python3.13/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/prajitrr/miniconda3/envs/chemsource/lib/python3.13/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/home/prajitrr/miniconda3/envs/chemsource/lib/python3.13/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/prajitrr/miniconda3/envs/chemsource/lib/python3.13/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "# Read in and Preprocess Drug Library dataset\n",
    "validation_dataset = pd.read_csv(\"../data/raw_validation_data/20240513_druglib_manual_class_with_synonyms.csv\")\n",
    "validation_dataset.drop(columns=[\"compound_name\"], inplace=True)\n",
    "validation_dataset.dropna(subset=[\"synonyms\"], inplace=True)\n",
    "validation_dataset[\"synonyms\"] = validation_dataset[\"synonyms\"].parallel_apply(lambda x: (preprocess_chemical(filter_synonym_list(try_literal_eval(x))))[:5])\n",
    "validation_dataset[\"synonyms\"] = validation_dataset[\"synonyms\"].apply(tuple)\n",
    "validation_dataset.drop_duplicates(subset=[\"synonyms\"], inplace=True)\n",
    "validation_dataset.drop(validation_dataset[validation_dataset[\"synonyms\"].apply(len) == 0].index, inplace=True)\n",
    "\n",
    "# Retrieve text\n",
    "validation_dataset[\"text\"] = validation_dataset[\"synonyms\"].parallel_apply(lambda x: list_retrieve(x, model))\n",
    "validation_dataset[\"name_used\"] = validation_dataset[\"text\"].apply(lambda x: x[0] if x else None)\n",
    "\n",
    "# Save to files\n",
    "validation_dataset.to_csv(\"../data/cleaned_data/cleaned_validation_data.csv\", index=False)\n",
    "validation_dataset_with_text = validation_dataset.dropna(subset=[\"text\"])\n",
    "validation_dataset_with_text.to_csv(\"../data/cleaned_data/cleaned_validation_data_with_text.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "datasets = [os.path.join(\"../data/raw_data\", x) for x in os.listdir(\"../data/raw_data\")]\n",
    "dataset_names = [\"iss\", \"mouse\", \"dust\", \"adrc_plasma\", \"rosmap\", \"adrc\"]\n",
    "\n",
    "# Combine datasets and label with names\n",
    "all_data = pd.DataFrame()\n",
    "for dataset in datasets:\n",
    "    data = pd.read_csv(dataset, on_bad_lines='skip', sep=\"\\t\")\n",
    "    data[\"dataset\"] = dataset_names[datasets.index(dataset)]\n",
    "    all_data = pd.concat([all_data, data])\n",
    "\n",
    "# Preprocess data\n",
    "all_data.drop(columns=[\"compound_name\"], inplace=True)\n",
    "all_data.dropna(subset=[\"synonyms\"], inplace=True)\n",
    "all_data[\"synonyms\"] = all_data[\"synonyms\"].parallel_apply(lambda x: (preprocess_chemical(filter_synonym_list(try_literal_eval(x))))[:5])\n",
    "all_data[\"synonyms\"] = all_data[\"synonyms\"].apply(tuple)\n",
    "all_data.drop_duplicates(subset=[\"synonyms\", \"dataset\"], inplace=True)\n",
    "all_data.drop(all_data[all_data[\"synonyms\"].apply(len) == 0].index, inplace=True)\n",
    "\n",
    "# Retrieve text\n",
    "all_data[\"text\"] = all_data[\"synonyms\"].parallel_apply(lambda x: list_retrieve(x, model))\n",
    "all_data[\"name_used\"] = all_data[\"text\"].apply(lambda x: x[0] if x else None)\n",
    "\n",
    "# Clean retrieved text and save data\n",
    "all_data[\"text\"] = all_data[\"text\"].apply(lambda x: x[1] if x else None)\n",
    "all_data.to_csv(\"../data/cleaned_data/all_cleaned_data.tsv\", index=False, sep=\"\\t\")\n",
    "all_data_with_text = all_data.dropna(subset=[\"text\"])\n",
    "all_data_with_text = all_data_with_text.drop(all_data_with_text[all_data_with_text[\"text\"] == (None, None)].index)\n",
    "all_data_with_text.to_csv(\"../data/cleaned_data/all_cleaned_data_with_text.tsv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Auxiliary Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf648cae19424cdc8f7076b79762a362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=4942), Label(value='0 / 4942'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208a28d1580247429c3d6cdba08e9385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=1084), Label(value='0 / 1084'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajitrr/miniconda3/envs/chemsource/lib/python3.13/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/prajitrr/miniconda3/envs/chemsource/lib/python3.13/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n",
      "/home/prajitrr/miniconda3/envs/chemsource/lib/python3.13/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/prajitrr/miniconda3/envs/chemsource/lib/python3.13/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "extra_controls = [\"../data/raw_data_frequencies/food_annotation_full_metadata_cleaned.tsv\",\n",
    "                  \"../data/raw_data_frequencies/PCP_annotation_full_metadata_cleaned.tsv\"]\n",
    "extra_controls_names = [\"food\", \"personal\"]\n",
    "\n",
    "extra_controls_data = pd.DataFrame()\n",
    "for dataset in extra_controls:\n",
    "    data = pd.read_csv(dataset, on_bad_lines='skip', sep=\"\\t\")\n",
    "    data[\"dataset\"] = extra_controls_names[extra_controls.index(dataset)]\n",
    "    extra_controls_data = pd.concat([extra_controls_data, data])\n",
    "\n",
    "# Preprocess data\n",
    "extra_controls_data.drop(columns=[\"compound_name\"], inplace=True)\n",
    "extra_controls_data.dropna(subset=[\"synonyms\"], inplace=True)\n",
    "extra_controls_data[\"synonyms\"] = extra_controls_data[\"synonyms\"].parallel_apply(lambda x: (preprocess_chemical(filter_synonym_list(try_literal_eval(x))))[:5])\n",
    "extra_controls_data[\"synonyms\"] = extra_controls_data[\"synonyms\"].apply(tuple)\n",
    "extra_controls_data.drop_duplicates(subset=[\"synonyms\", \"dataset\"], inplace=True)\n",
    "extra_controls_data.drop(extra_controls_data[extra_controls_data[\"synonyms\"].apply(len) == 0].index, inplace=True)\n",
    "# Retrieve text\n",
    "extra_controls_data[\"text\"] = extra_controls_data[\"synonyms\"].parallel_apply(lambda x: list_retrieve(x, model))\n",
    "extra_controls_data[\"name_used\"] = extra_controls_data[\"text\"].apply(lambda x: x[0] if x else None)\n",
    "# Clean retrieved text and save data\n",
    "extra_controls_data[\"text\"] = extra_controls_data[\"text\"].apply(lambda x: x[1] if x else None)\n",
    "extra_controls_data.to_csv(\"../data/cleaned_data/extra_controls_cleaned_data.tsv\", index=False, sep=\"\\t\")\n",
    "extra_controls_data_with_text = extra_controls_data.dropna(subset=[\"text\"])\n",
    "extra_controls_data_with_text = extra_controls_data_with_text.drop(extra_controls_data_with_text[extra_controls_data_with_text[\"text\"] == (None, None)].index)\n",
    "extra_controls_data_with_text.to_csv(\"../data/cleaned_data/extra_controls_cleaned_data_with_text.tsv\", index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_datasets = [os.path.join(\"../data/raw_data_frequencies\", x) for x in os.listdir(\"../data/raw_data_frequencies\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_datasets = [os.path.join(\"../data/raw_data_frequencies\", x) for x in os.listdir(\"../data/raw_data_frequencies\")]\n",
    "freq_dataset_names = [\"adrc_plasma\", \"food\", \"iss\",\"adrc\",\"mouse\", \"rosmap\", \"personal\",\"dust\"]\n",
    "\n",
    "# Combine datasets and label with names\n",
    "all_freq_data = pd.DataFrame()\n",
    "for dataset in frequency_datasets:\n",
    "    if dataset.endswith(\".csv\"):\n",
    "        data = pd.read_csv(dataset, on_bad_lines='skip')\n",
    "    else:\n",
    "        data = pd.read_csv(dataset, on_bad_lines='skip', sep=\"\\t\")    \n",
    "    data[\"dataset\"] = freq_dataset_names[frequency_datasets.index(dataset)]\n",
    "    all_freq_data = pd.concat([all_freq_data, data])\n",
    "all_freq_data.to_csv(\"../data/cleaned_data/all_cleaned_data_frequencies.tsv\", index=False, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemsource",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
